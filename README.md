### Learn optimizers by coding

Implementation of various gradient descent based optimizers in native python. Using the example of linear regression and its loss function, we implement various optimizers in raw python to get a thorough understanding of how they work and the differences between the optimizers. 

Optimizers implemented include:

* Gradient Descent
* Mini batch gradient descent
* Momentum
* Nesterov
* Adagrad
* Adadelta
* RMSProp
* Adam
* Adamax
* Nadam

Equations and descriptions of optimizers is also included in the notebook. 

Based on the [paper](https://arxiv.org/pdf/1609.04747.pdf)/ [blog post](http://ruder.io/optimizing-gradient-descent/) by [Sebastian Ruder](http://ruder.io/).

### License

MIT License
